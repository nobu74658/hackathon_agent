from typing import Dict, Any, List, Optional, Tuple
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain.schema.runnable import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from datetime import datetime
import json

from app.services.conversation_memory import ConversationMemoryService
from app.core.config import settings


class QuestionResponse(BaseModel):
    """è³ªå•ç”Ÿæˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ """
    questions: List[str] = Field(description="ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã®ãƒªã‚¹ãƒˆ")
    reasoning: str = Field(description="ãªãœã“ã‚Œã‚‰ã®è³ªå•ãŒå¿…è¦ã‹ã®èª¬æ˜")
    information_needed: List[str] = Field(description="ã¾ã å¿…è¦ãªæƒ…å ±ã®ãƒªã‚¹ãƒˆ")
    completeness_score: int = Field(description="æƒ…å ±ã®å……è¶³åº¦ï¼ˆ0-100ï¼‰")


class ActionPlanResponse(BaseModel):
    """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ """
    action_items: List[Dict[str, Any]] = Field(description="ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ")
    summary: str = Field(description="å…¨ä½“çš„ãªã‚µãƒãƒªãƒ¼")
    key_improvements: List[str] = Field(description="ä¸»è¦ãªæ”¹å–„ãƒã‚¤ãƒ³ãƒˆ")
    metrics: Dict[str, Any] = Field(description="æˆåŠŸæŒ‡æ¨™")


class DialogueManager:
    """å¯¾è©±ãƒ•ãƒ­ãƒ¼ã‚’ç®¡ç†ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def __init__(self):
        self.memory_service = ConversationMemoryService()
        self.llm = ChatOpenAI(
            model=settings.OPENAI_MODEL,
            temperature=0.7,
            api_key=settings.OPENAI_API_KEY
        )
        self.question_parser = PydanticOutputParser(pydantic_object=QuestionResponse)
        self.action_plan_parser = PydanticOutputParser(pydantic_object=ActionPlanResponse)
    
    async def initialize(self):
        """ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        await self.memory_service.initialize()
    
    async def start_dialogue(
        self,
        session_id: str,
        initial_context: Dict[str, Any]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹"""
        # åˆæœŸè³ªå•ç”Ÿæˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ã‚ãªãŸã¯æ–°äººå–¶æ¥­ãƒãƒ³ã®æˆé•·ã‚’æ”¯æ´ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            1on1ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å†…å®¹ã‹ã‚‰ã€å–¶æ¥­ã‚¹ã‚­ãƒ«å‘ä¸Šã®ãŸã‚ã®å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«
            å¿…è¦ãªæƒ…å ±ã‚’åé›†ã—ã¾ã™ã€‚
            
            ä»¥ä¸‹ã®æƒ…å ±ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼š
            {initial_context}
            
            åŠ¹æœçš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«è¿½åŠ ã§å¿…è¦ãªæƒ…å ±ã‚’ç‰¹å®šã—ã€
            3-5å€‹ã®å…·ä½“çš„ã§ç­”ãˆã‚„ã™ã„è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            
            {format_instructions}
            """),
            ("user", "1on1ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å†…å®¹ã‚’åˆ†æã—ã€è¿½åŠ ã§å¿…è¦ãªæƒ…å ±ã‚’åé›†ã™ã‚‹ãŸã‚ã®è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        ])
        
        # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
        chain = (
            {
                "initial_context": RunnablePassthrough(),
                "format_instructions": lambda _: self.question_parser.get_format_instructions()
            }
            | prompt
            | self.llm
            | self.question_parser
        )
        
        # è³ªå•ç”Ÿæˆ
        response = await chain.ainvoke(json.dumps(initial_context, ensure_ascii=False))
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        metadata = {
            "stage": "initial",
            "reasoning": response.reasoning,
            "information_needed": response.information_needed,
            "completeness_score": response.completeness_score
        }
        
        return response.questions, metadata
    
    async def process_user_response(
        self,
        session_id: str,
        user_response: str,
        db_session: Any
    ) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’å‡¦ç†"""
        # ãƒ¡ãƒ¢ãƒªã«å›ç­”ã‚’è¿½åŠ 
        await self.memory_service.add_message(
            session_id=session_id,
            role="user",
            content=user_response,
            db=db_session
        )
        
        # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        context = await self.memory_service.get_conversation_context(
            session_id=session_id,
            include_summary=True
        )
        
        # æƒ…å ±ã®å……è¶³åº¦ã‚’è©•ä¾¡
        completeness_score = await self._evaluate_completeness(context)
        
        # å¯¾è©±ã®æ®µéšã‚’åˆ¤å®š
        dialogue_stage = self._determine_dialogue_stage(context, completeness_score)
        
        if dialogue_stage == "action_plan" and completeness_score >= 95:
            # ååˆ†ãªæƒ…å ±ãŒé›†ã¾ã£ãŸå ´åˆã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆ
            action_plan = await self._generate_action_plan(context)
            return {
                "type": "action_plan",
                "data": action_plan,
                "completeness_score": completeness_score,
                "stage": dialogue_stage
            }
        else:
            # ã¾ã æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã€æ®µéšçš„ãªè³ªå•ç”Ÿæˆ
            follow_up_questions = await self._generate_stage_based_questions(context, dialogue_stage)
            return {
                "type": "follow_up",
                "questions": follow_up_questions,
                "completeness_score": completeness_score,
                "stage": dialogue_stage,
                "stage_description": self._get_stage_description(dialogue_stage)
            }
    
    def _determine_dialogue_stage(self, context: Dict[str, Any], completeness_score: int) -> str:
        """å¯¾è©±ã®æ®µéšã‚’åˆ¤å®š"""
        messages = context.get("messages", [])
        message_count = len([msg for msg in messages if msg.get("role") == "user"])
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã¨å®Œäº†åº¦ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹æ®µéšåˆ¤å®š
        if message_count <= 1:
            return "initial_understanding"  # åˆæœŸç†è§£æ®µéš
        elif message_count <= 3 and completeness_score < 40:
            return "problem_clarification"  # èª²é¡Œæ˜ç¢ºåŒ–æ®µéš  
        elif message_count <= 5 and completeness_score < 70:
            return "deep_analysis"  # æ·±ã„åˆ†ææ®µéš
        elif completeness_score < 85:
            return "solution_exploration"  # è§£æ±ºç­–æ¢ç´¢æ®µéš
        elif completeness_score < 95:
            return "action_preparation"  # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æº–å‚™æ®µéš
        else:
            return "action_plan"  # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆæ®µéš
    
    def _get_stage_description(self, stage: str) -> str:
        """æ®µéšã®èª¬æ˜ã‚’å–å¾—"""
        descriptions = {
            "initial_understanding": "ğŸ“‹ åˆæœŸçŠ¶æ³ã®ç†è§£",
            "problem_clarification": "ğŸ¯ èª²é¡Œã®æ˜ç¢ºåŒ–", 
            "deep_analysis": "ğŸ” è©³ç´°åˆ†æ",
            "solution_exploration": "ğŸ’¡ è§£æ±ºç­–ã®æ¢ç´¢",
            "action_preparation": "ğŸ“ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æº–å‚™",
            "action_plan": "ğŸš€ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆ"
        }
        return descriptions.get(stage, "ğŸ¤” åˆ†æä¸­")
    
    async def _generate_stage_based_questions(self, context: Dict[str, Any], stage: str) -> List[str]:
        """æ®µéšã«åŸºã¥ã„ãŸè³ªå•ç”Ÿæˆ"""
        if stage == "initial_understanding":
            return await self._generate_initial_questions(context)
        elif stage == "problem_clarification":
            return await self._generate_clarification_questions(context)
        elif stage == "deep_analysis":
            return await self._generate_analysis_questions(context)
        elif stage == "solution_exploration":
            return await self._generate_solution_questions(context)
        elif stage == "action_preparation":
            return await self._generate_preparation_questions(context)
        else:
            return await self._generate_follow_up_questions(context)
    
    async def _generate_initial_questions(self, context: Dict[str, Any]) -> List[str]:
        """åˆæœŸç†è§£ã®ãŸã‚ã®è³ªå•ç”Ÿæˆ"""
        messages = context.get("messages", [])
        latest_message = messages[-1].get("content", "") if messages else ""
        
        # 1on1ã®å†…å®¹ã‹ã‚‰æŠ½è±¡çš„ãªæŒ‡ç¤ºã‚’ç‰¹å®š
        if "è·é›¢ã‚’è©°ã‚ã‚‹" in latest_message or "ä¿¡é ¼é–¢ä¿‚" in latest_message:
            return [
                "ä¸Šå¸ã‹ã‚‰ã€Œé¡§å®¢ã¨ã®è·é›¢ã‚’è©°ã‚ã‚‹ã€ã¨ã„ã†æŒ‡æ‘˜ãŒã‚ã‚Šã¾ã—ãŸãŒã€å…·ä½“çš„ã«ã©ã®ã‚ˆã†ãªå ´é¢ã§ãã†æ„Ÿã˜ã‚‰ã‚ŒãŸã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ",
                "ã“ã‚Œã¾ã§ã®å–¶æ¥­æ´»å‹•ã§ã€é¡§å®¢ã¨ã®é–¢ä¿‚æ§‹ç¯‰ã«ãŠã„ã¦æœ€ã‚‚å›°é›£ã ã£ãŸç¬é–“ã¯ã©ã‚“ãªæ™‚ã§ã—ãŸã‹ï¼Ÿ",
                "ç¾åœ¨ã€æ–°è¦é¡§å®¢ã¨ã®ã‚„ã‚Šå–ã‚Šã§ç‰¹ã«æ„è­˜ã—ã¦ã„ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            ]
        elif "ç›¸æ‰‹ã®èª²é¡Œã«å¯„ã‚Šæ·»ã£ãŸææ¡ˆ" in latest_message:
            return [
                "ã€Œç›¸æ‰‹ã®èª²é¡Œã«å¯„ã‚Šæ·»ã£ãŸææ¡ˆã€ã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§ã©ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å–ã‚‰ã‚Œã¦ã„ã¾ã—ãŸã‹ï¼Ÿ",
                "é¡§å®¢ã®ãƒ’ã‚¢ãƒªãƒ³ã‚°æ™‚ã«ã€ã©ã‚“ãªè³ªå•ã‚’ã™ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã‹ï¼Ÿ",
                "ææ¡ˆå†…å®¹ã‚’æ±ºã‚ã‚‹éš›ã«ã€æœ€ã‚‚é‡è¦–ã—ã¦ã„ã‚‹ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ"
            ]
        elif "æ¸©åº¦æ„Ÿã‚’èª­ã‚€" in latest_message:
            return [
                "ä¸Šå¸ã‹ã‚‰ã€Œé¡§å®¢ã®æ¸©åº¦æ„Ÿã‚’èª­ã‚€ã€ã¨ã„ã†æŒ‡æ‘˜ãŒã‚ã‚Šã¾ã—ãŸãŒã€ã“ã‚Œã¾ã§ã«ãã†ã„ã£ãŸæ„Ÿè¦šã‚’æ„è­˜ã—ãŸã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
                "å•†è«‡ä¸­ã«ã€é¡§å®¢ã®èˆˆå‘³ã‚„é–¢å¿ƒåº¦ã‚’ã©ã®ã‚ˆã†ã«åˆ¤æ–­ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ",
                "é¡§å®¢ãŒç©æ¥µçš„ãªæ™‚ã¨æ¶ˆæ¥µçš„ãªæ™‚ã®é•ã„ã‚’ã€ä½•ã§æ„Ÿã˜å–ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã‹ï¼Ÿ"
            ]
        else:
            return [
                "ã“ã®1on1ã®å†…å®¹ã«ã¤ã„ã¦ã€ã©ã®éƒ¨åˆ†ãŒæœ€ã‚‚æ”¹å–„ã—ãŸã„ãƒã‚¤ãƒ³ãƒˆã ã¨æ„Ÿã˜ã¾ã™ã‹ï¼Ÿ",
                "ä¸Šå¸ã‹ã‚‰ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ã€ç‰¹ã«å…·ä½“çš„ãªæ–¹æ³•ã‚’çŸ¥ã‚ŠãŸã„ã¨æ€ã£ãŸéƒ¨åˆ†ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
                "ç¾åœ¨ã®å–¶æ¥­æ´»å‹•ã§ã€æœ€ã‚‚ä¸å®‰ã«æ„Ÿã˜ã¦ã„ã‚‹ã“ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
            ]
    
    async def _generate_clarification_questions(self, context: Dict[str, Any]) -> List[str]:
        """èª²é¡Œæ˜ç¢ºåŒ–ã®ãŸã‚ã®è³ªå•ç”Ÿæˆ"""
        return [
            "ãã®èª²é¡ŒãŒç™ºç”Ÿã™ã‚‹å…¸å‹çš„ãªã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å…·ä½“çš„ã«æ•™ãˆã¦ãã ã•ã„",
            "åŒã˜ã‚ˆã†ãªçŠ¶æ³ã§ã€ã†ã¾ãã„ã£ãŸçµŒé¨“ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿãã®æ™‚ã¯ä½•ãŒé•ã„ã¾ã—ãŸã‹ï¼Ÿ",
            "ã“ã®èª²é¡Œã«ã‚ˆã£ã¦ã€å®Ÿéš›ã«ã©ã®ã‚ˆã†ãªæå¤±ã‚„æ©Ÿä¼šæå¤±ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã‹ï¼Ÿ"
        ]
    
    async def _generate_analysis_questions(self, context: Dict[str, Any]) -> List[str]:
        """æ·±ã„åˆ†æã®ãŸã‚ã®è³ªå•ç”Ÿæˆ"""
        return [
            "ãã®èª²é¡Œã®æ ¹æœ¬çš„ãªåŸå› ã¯ä½•ã ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ",
            "ã“ã‚Œã¾ã§ã«è©¦ã—ãŸã“ã¨ãŒã‚ã‚‹è§£æ±ºç­–ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„",
            "ç†æƒ³çš„ãªçŠ¶æ…‹ã«ãªã£ãŸã¨ãã€ã©ã®ã‚ˆã†ãªå¤‰åŒ–ãŒæœŸå¾…ã§ãã¾ã™ã‹ï¼Ÿ"
        ]
    
    async def _generate_solution_questions(self, context: Dict[str, Any]) -> List[str]:
        """è§£æ±ºç­–æ¢ç´¢ã®ãŸã‚ã®è³ªå•ç”Ÿæˆ"""
        return [
            "è§£æ±ºç­–ã‚’å®Ÿè¡Œã™ã‚‹ä¸Šã§ã€ã©ã®ã‚ˆã†ãªã‚µãƒãƒ¼ãƒˆã‚„è³‡æºãŒå¿…è¦ã§ã™ã‹ï¼Ÿ",
            "ã“ã®æ”¹å–„ã«å–ã‚Šçµ„ã‚€éš›ã®æœŸé™ã‚„ç›®æ¨™ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "æˆåŠŸã‚’æ¸¬ã‚‹å…·ä½“çš„ãªæŒ‡æ¨™ãŒã‚ã‚Œã°æ•™ãˆã¦ãã ã•ã„"
        ]
    
    async def _generate_preparation_questions(self, context: Dict[str, Any]) -> List[str]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æº–å‚™ã®ãŸã‚ã®è³ªå•ç”Ÿæˆ"""
        return [
            "å®Ÿè¡Œã«ç§»ã™å‰ã«ä¸å®‰ãªç‚¹ã‚„æ‡¸å¿µã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "å–ã‚Šçµ„ã¿ã‚’å§‹ã‚ã‚‹æœ€é©ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¯ã„ã¤é ƒã§ã—ã‚‡ã†ã‹ï¼Ÿ",
            "é€²æ—ã‚’ç¢ºèªã™ã‚‹æ–¹æ³•ã‚„é »åº¦ã¯ã©ã†ã—ã¾ã™ã‹ï¼Ÿ"
        ]
    
    async def _evaluate_completeness(self, context: Dict[str, Any]) -> int:
        """æƒ…å ±ã®å……è¶³åº¦ã‚’è©•ä¾¡"""
        context_str = json.dumps(context, ensure_ascii=False, indent=2)
        messages = [
            {"role": "system", "content": """å–¶æ¥­ã‚¹ã‚­ãƒ«å‘ä¸Šã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ä½œæˆã«å¿…è¦ãªæƒ…å ±ã®å……è¶³åº¦ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

æ®µéšçš„è©•ä¾¡åŸºæº–ï¼š
ã€20-30ç‚¹ã€‘èª²é¡Œã®æ¦‚è¦ã¯ç†è§£ã§ãã‚‹ãŒã€å…·ä½“æ€§ã«æ¬ ã‘ã‚‹
ã€40-50ç‚¹ã€‘èª²é¡Œã¯æ˜ç¢ºã ãŒã€å…·ä½“çš„ãªçŠ¶æ³ã‚„èƒŒæ™¯æƒ…å ±ãŒä¸è¶³
ã€60-70ç‚¹ã€‘èª²é¡Œã¨çŠ¶æ³ã¯æ˜ç¢ºã ãŒã€è©³ç´°ãªåˆ†æãŒå¿…è¦
ã€80-85ç‚¹ã€‘è©³ç´°ã¯æƒã£ã¦ã„ã‚‹ãŒã€è§£æ±ºç­–ã®æ–¹å‘æ€§ãŒæœªç¢ºå®š
ã€90-95ç‚¹ã€‘å…¨ã¦ã®æƒ…å ±ãŒæƒã„ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ä½œæˆæº–å‚™å®Œäº†
ã€95ç‚¹ä»¥ä¸Šã€‘ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆã«ååˆ†ãªæƒ…å ±

ä»¥ä¸‹ã®è¦³ç‚¹ã§å³æ ¼ã«è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
1. èª²é¡Œã®å…·ä½“æ€§ï¼ˆæŠ½è±¡çš„ãªæŒ‡æ‘˜â†’å…·ä½“çš„ãªå ´é¢ï¼‰
2. æ ¹æœ¬åŸå› ã®æŠŠæ¡ï¼ˆç—‡çŠ¶â†’åŸå› ã®ç‰¹å®šï¼‰
3. ç›®æ¨™ã¨æœŸé™ã®æ˜ç¢ºåŒ–
4. å®Ÿè¡Œå¯èƒ½æ€§ã®æ¤œè¨¼
5. æˆåŠŸæŒ‡æ¨™ã®å®šç¾©

0-100ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚æ•°å­—ã®ã¿å›ç­”ã—ã¦ãã ã•ã„ã€‚"""},
            {"role": "user", "content": f"ä¼šè©±å±¥æ­´ï¼š\n{context_str}\n\nå……è¶³åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰:"}
        ]
        
        from langchain.schema import HumanMessage, SystemMessage
        langchain_messages = [
            SystemMessage(content=messages[0]["content"]),
            HumanMessage(content=messages[1]["content"])
        ]
        response = await self.llm.ainvoke(langchain_messages)
        try:
            score = int(response.content.strip())
            return min(max(score, 0), 100)  # 0-100ã®ç¯„å›²ã«åˆ¶é™
        except:
            return 50  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    async def _generate_follow_up_questions(
        self,
        context: Dict[str, Any]
    ) -> List[str]:
        """ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ã‚’ç”Ÿæˆ"""
        # ãƒ¡ãƒ¢ãƒªã‹ã‚‰å±¥æ­´ã‚’å–å¾—
        memory = await self.memory_service.get_or_create_memory(context["session_id"])
        messages = memory.chat_memory.messages
        
        # ä¼šè©±å±¥æ­´ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        chat_history = "\n".join([
            f"{msg.type}: {msg.content}" for msg in messages[-5:]  # æœ€æ–°5ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ])
        
        prompt_messages = [
            {"role": "system", "content": """ã“ã‚Œã¾ã§ã®ä¼šè©±å†…å®¹ã‚’è¸ã¾ãˆã¦ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ä½œæˆã«å¿…è¦ãªè¿½åŠ æƒ…å ±ã‚’åé›†ã™ã‚‹ãŸã‚ã®è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            
            ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š
            1. ã™ã§ã«å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã‚’è¸ã¾ãˆã¦ã€ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã™ã‚‹
            2. å®Ÿè·µçš„ã§æ¸¬å®šå¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ã¤ãªãŒã‚‹æƒ…å ±ã‚’åé›†ã™ã‚‹
            3. å–¶æ¥­ã‚¹ã‚­ãƒ«å‘ä¸Šã«ç›´æ¥é–¢é€£ã™ã‚‹è³ªå•ã‚’ã™ã‚‹
            
            è³ªå•ã¯1è¡Œãšã¤ã€ŒQ: ã€ã§å§‹ã‚ã¦3-5å€‹å›ç­”ã—ã¦ãã ã•ã„ã€‚"""},
            {"role": "user", "content": f"ä¼šè©±å±¥æ­´ï¼š\n{chat_history}\n\nè¿½åŠ ã§å¿…è¦ãªæƒ…å ±ã‚’åé›†ã™ã‚‹ãŸã‚ã®è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"}
        ]
        
        langchain_messages = [
            SystemMessage(content=prompt_messages[0]["content"]),
            HumanMessage(content=prompt_messages[1]["content"])
        ]
        response = await self.llm.ainvoke(langchain_messages)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰è³ªå•ã‚’æŠ½å‡º
        questions = []
        for line in response.content.split('\n'):
            line = line.strip()
            if line.startswith('Q:'):
                questions.append(line[2:].strip())
            elif line.startswith('è³ªå•'):
                questions.append(line.split(':', 1)[-1].strip())
        
        # æœ€ä½1ã¤ã®è³ªå•ã‚’ä¿è¨¼
        if not questions:
            questions = ["ã“ã‚Œã¾ã§ã®å†…å®¹ã«ã¤ã„ã¦ã€ã‚‚ã†å°‘ã—è©³ã—ãæ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ"]
        
        return questions[:5]  # æœ€å¤§5å€‹ã¾ã§
    
    async def _generate_action_plan(
        self,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’ç”Ÿæˆ"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä¼šè©±å†…å®¹ã‚’åŸºã«ã€æ–°äººå–¶æ¥­ãƒãƒ³ã®æˆé•·ã®ãŸã‚ã®
            å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            
            ä»¥ä¸‹ã®è¦ç´ ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
            1. å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
            2. å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æœŸé™ã¨æˆåŠŸæŒ‡æ¨™
            3. å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ã‚„ã‚µãƒãƒ¼ãƒˆ
            4. æœŸå¾…ã•ã‚Œã‚‹æˆæœ
            
            {format_instructions}
            """),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "ã“ã‚Œã¾ã§ã®ä¼šè©±å†…å®¹ã‚’åŸºã«ã€æˆé•·æ”¯æ´ã®ãŸã‚ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        ])
        
        memory = await self.memory_service.get_or_create_memory(context["session_id"])
        messages = memory.chat_memory.messages
        
        chain = (
            {
                "chat_history": lambda _: messages,
                "format_instructions": lambda _: self.action_plan_parser.get_format_instructions()
            }
            | prompt
            | self.llm
            | self.action_plan_parser
        )
        
        response = await chain.ainvoke({})
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        return {
            "action_items": response.action_items,
            "summary": response.summary,
            "key_improvements": response.key_improvements,
            "metrics": response.metrics,
            "generated_at": datetime.utcnow().isoformat()
        }
    
    async def save_dialogue_state(
        self,
        session_id: str,
        state: str,
        metadata: Dict[str, Any],
        db_session: Any
    ) -> None:
        """å¯¾è©±ã®çŠ¶æ…‹ã‚’ä¿å­˜"""
        # Redisã«çŠ¶æ…‹ã‚’ä¿å­˜
        state_key = f"dialogue:state:{session_id}"
        state_data = {
            "state": state,
            "metadata": metadata,
            "updated_at": datetime.utcnow().isoformat()
        }
        
        if self.memory_service.redis_client:
            await self.memory_service.redis_client.setex(
                state_key,
                86400,  # 24æ™‚é–“
                json.dumps(state_data, ensure_ascii=False)
            )
    
    async def get_dialogue_state(
        self,
        session_id: str
    ) -> Optional[Dict[str, Any]]:
        """å¯¾è©±ã®çŠ¶æ…‹ã‚’å–å¾—"""
        if not self.memory_service.redis_client:
            return None
        
        state_key = f"dialogue:state:{session_id}"
        state_data = await self.memory_service.redis_client.get(state_key)
        
        if state_data:
            return json.loads(state_data)
        return None